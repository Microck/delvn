---
phase: 05-demo-submit
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - demo/config.yaml
  - demo/run_demo.py
  - docs/example_brief.md
autonomous: true
must_haves:
  truths:
    - "A single demo script runs the end-to-end flow (collect -> correlate -> prioritize -> report)"
    - "Demo runner supports a non-destructive `--dry-run` that does not call external APIs or write to Azure"
    - "A sample executive brief exists for screenshots/readme"
  artifacts:
    - path: "demo/run_demo.py"
      provides: "Demo runner entrypoint"
    - path: "demo/config.yaml"
      provides: "Demo config (stack + run parameters)"
    - path: "docs/example_brief.md"
      provides: "Example output used in submission"
  key_links:
    - from: "demo/run_demo.py"
      to: "src/agents/*"
      via: "calls into agent entrypoints"
      pattern: "run_"
---

<objective>
Create demo assets and a single script to run the full pipeline.

Purpose: Make the 2-minute demo repeatable and resilient under time pressure.
Output: demo config + runner + example brief output.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add demo config</name>
  <files>demo/config.yaml</files>
  <action>
Create `demo/config.yaml` including:
- user stack (Apache, PostgreSQL, React)
- collection window (e.g., last N days)
- max items per source
- correlation topK

Keep it small and readable; this file is used on-screen in the demo.
  </action>
  <verify>python3 -m pip install -e . && python3 -c "import pathlib, yaml; yaml.safe_load(pathlib.Path('demo/config.yaml').read_text())"</verify>
  <done>demo/config.yaml exists and parses as YAML</done>
</task>

<task type="auto">
  <name>Task 2: Implement demo runner script</name>
  <files>demo/run_demo.py</files>
  <action>
Create `demo/run_demo.py`:
- Load demo config
- Run collectors (CVE, Intel, News)
- Run correlator
- Run prioritizer
- Run reporter to produce markdown
- Print a short console summary (counts + top headlines)

Make it safe to run in judging environments:
- Add `--dry-run` that only loads/validates config and prints the steps that would run; it must NOT call external APIs and must NOT write to Cosmos/Search.
- Keep the live behavior behind explicit flags (e.g., `--live`) or behind presence of required env vars.
  </action>
  <verify>python3 -m pip install -e . && python3 demo/run_demo.py --dry-run</verify>
  <done>`--dry-run` exits 0 and is non-destructive; module is importable</done>
</task>

<task type="auto">
  <name>Task 3: Generate and commit a sample executive brief markdown</name>
  <files>docs/example_brief.md</files>
  <action>
Add `docs/example_brief.md` with a representative output structure:
- Executive summary
- Top risks (3-5)
- Notable mentions
- Recommended next steps

This can be generated from real data once the pipeline works; for now, include realistic placeholder content aligned with the demo scenario.
  </action>
  <verify>test -f docs/example_brief.md</verify>
  <done>Example brief file exists</done>
</task>

</tasks>

<verification>
- `python3 demo/run_demo.py --dry-run` exits 0 without making network calls or writing to Azure
</verification>

<success_criteria>
- Running the demo becomes a single command that produces a brief
</success_criteria>

<output>
After completion, create `.planning/phases/05-demo-submit/05-01-SUMMARY.md`
</output>
