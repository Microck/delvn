---
phase: 01-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - infra/foundry_config.yaml
  - scripts/smoke_azure_storage.py
  - src/storage/__init__.py
  - src/storage/cosmos.py
  - src/storage/search.py
  - tests/test_storage_clients_smoke.py
autonomous: true
user_setup:
  - service: azure
    why: "Cosmos DB + Azure AI Search access"
    env_vars:
      - name: COSMOS_ENDPOINT
        source: "Azure Portal/CLI after creating Cosmos account"
      - name: COSMOS_KEY
        source: "Azure Portal/CLI (Cosmos keys)"
      - name: SEARCH_ENDPOINT
        source: "Azure Portal/CLI after creating Search service"
      - name: SEARCH_KEY
        source: "Azure Portal/CLI (Search admin key)"
  - service: azure-ai-foundry
    why: "Optional: Azure AI Foundry / Azure OpenAI project for embeddings used in Phase 3"
    env_vars:
      - name: AZURE_OPENAI_ENDPOINT
        source: "Azure AI Foundry / Azure OpenAI deployment endpoint"
      - name: AZURE_OPENAI_API_KEY
        source: "Azure AI Foundry / Azure OpenAI API key"
      - name: AZURE_OPENAI_EMBEDDING_DEPLOYMENT
        source: "Embedding deployment name"
      - name: AZURE_OPENAI_API_VERSION
        source: "Azure OpenAI REST API version (used by smoke script)"
must_haves:
  truths:
    - "Storage adapters import without making live network calls"
    - "(Optional live) With Cosmos env vars set, app can ensure containers and upsert/read a sample threat"
    - "(Optional live) With Search env vars set, app can ensure the Search index exists"
  artifacts:
    - path: "src/storage/cosmos.py"
      provides: "Cosmos DB client wrapper for threat storage"
    - path: "src/storage/search.py"
      provides: "Azure AI Search client wrapper + index bootstrap"
    - path: "infra/foundry_config.yaml"
      provides: "Foundry/infra config placeholders tracked in repo"
    - path: "scripts/smoke_azure_storage.py"
      provides: "Optional live smoke checks for Cosmos/Search/Foundry env"
  key_links:
    - from: "src/storage/cosmos.py"
      to: "src/config/settings.py"
      via: "get_settings()"
      pattern: "COSMOS_"
    - from: "src/storage/search.py"
      to: "src/config/settings.py"
      via: "get_settings()"
      pattern: "SEARCH_"
---

<objective>
Add storage adapters for Cosmos DB and Azure AI Search, plus minimal infra config placeholders.

Purpose: Enable all agents to persist threats and later run vector search for correlation.
Output: Cosmos + Search wrappers; Foundry config file.
</objective>

<execution_context>
@/home/ubuntu/.config/opencode/get-shit-done/workflows/execute-plan.md
@/home/ubuntu/.config/opencode/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Cosmos DB threat store wrapper</name>
  <files>
 src/storage/__init__.py
 src/storage/cosmos.py
  </files>
  <action>
Create `src/storage/cosmos.py` using `azure.cosmos`.

Implement a small wrapper (no heavy abstraction):
- `CosmosStore` initialized from `config.settings.get_settings()`
- `ensure_database_and_containers()` creates database + containers if missing (threats + correlations)
- `upsert_threat(doc: dict) -> dict`
- `get_threat(id: str, partition_key: str) -> dict | None`

Partitioning:
- Use `/source` as partition key for the `threats` container to keep writes simple.
- Store full document including `id`, `source`, `type`, `title`, `raw`.

Keep auth simple: key-based client via env vars.
  </action>
  <verify>python3 -c "from storage.cosmos import CosmosStore; CosmosStore"</verify>
  <done>Module imports; live CRUD is exercised via optional smoke script when env vars are present</done>
</task>

<task type="auto">
  <name>Task 2: Implement Azure AI Search wrapper with index bootstrap</name>
  <files>src/storage/search.py</files>
  <action>
  Create `src/storage/search.py` using `azure.search.documents`.

  Implement:
  - `SearchStore` initialized from settings
  - `ensure_index(index_name: str, *, embedding_dim: int)` that creates/updates an index that can later support vector search
  - `upsert_documents(docs: list[dict])`
  - `search_text(query: str, *, top: int = 5) -> list[dict]` for smoke/demo queries (no vector query yet)

Index schema minimum:
- key field `id` (Edm.String)
- filterable fields: `source`, `type`, `published_at`
- searchable text field: `content`
- vector field: `contentVector` (collection of single) with dimension from settings

  Do NOT overbuild correlation fields yet; keep it threat-document focused.
  </action>
  <verify>python3 -c "from storage.search import SearchStore; assert hasattr(SearchStore, 'ensure_index') and hasattr(SearchStore, 'search_text')"</verify>
  <done>Search wrapper supports ensure_index + a basic text query method for live smoke validation</done>
</task>

<task type="auto">
  <name>Task 3: Add infra placeholders + optional live smoke script + import-only smoke tests</name>
  <files>
 infra/foundry_config.yaml
 scripts/smoke_azure_storage.py
 tests/test_storage_clients_smoke.py
  </files>
  <action>
Add `infra/foundry_config.yaml` containing placeholders for:
- resource group name
- cosmos account name
- search service name
- deployment region

  Add `scripts/smoke_azure_storage.py` as a non-destructive smoke runner:
  - Default behavior: print which env vars are missing and exit 0 (SKIP).
  - If `COSMOS_ENDPOINT` + `COSMOS_KEY` are set: call `CosmosStore.ensure_database_and_containers()`, upsert a tiny sample threat doc, then read it back.
  - If `SEARCH_ENDPOINT` + `SEARCH_KEY` are set:
    - call `SearchStore.ensure_index(...)`
    - upsert a tiny sample doc with distinctive content (e.g., "tf-smoke")
    - run `SearchStore.search_text("tf-smoke")` and assert the upserted id is present
  - If Foundry/Azure OpenAI env vars (`AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_EMBEDDING_DEPLOYMENT`, `AZURE_OPENAI_API_VERSION`) are present:
    - perform a single embeddings REST call for input text "tf-smoke" and assert a vector-like response is returned
    - if any are missing, print SKIP (do not fail)

Add `tests/test_storage_clients_smoke.py` that only imports storage modules (no live Azure calls).
  </action>
  <verify>python3 -m pytest -q</verify>
  <done>Import-only smoke tests pass without Azure credentials; optional live smoke can be run when env vars exist</done>
</task>

</tasks>

<verification>
  - `python3 -m pytest -q` passes without Azure credentials
  - (Optional live) `python3 scripts/smoke_azure_storage.py` performs:
    - Cosmos: ensure containers + upsert + read
    - Search: ensure_index + upsert + query (text)
    - Foundry: embeddings request when configured
</verification>

<success_criteria>
- Storage modules are ready to be used by agents, with all connection details centralized in settings
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
